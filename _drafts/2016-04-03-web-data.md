---
name: web-data
layout: post
title: Web databases
date: 2016-04-03
authors:
  - name: Scott Chamberlain
tags:
- R
- database
- open data
- API
---

There is a lot of data on the web for use in academic, government, non-profit, or private sector use. Where in fact is this data? In what form?  What are the benefits of data being available on the web, and what are the caveats? I'll tackle these questions and more.

## <a href="#intdb" name="intdb">#</a> Databases on the web

First, some defintions. A _database_ is typically one or more tables of data, often connected to one another through common fields to allow fast queries. Databases are often defined as structured sets of data. This is in contrast to the most common bucket of data, the _spreadsheet_ or _dataset_, most commonly as an Excel file (`.xls`, `.xlsx`), or as a `.csv` file. These files hold data, but are not databases themselves. Putting a number of single spreadsheets together in e.g. PostgreSQL creates a database. Databases come in two flavors: SQL and No-SQL. SQL databases are collections of row-column oriented data (i.e. tables) connected to one another by similar columns. These include [MySQL][mysql], [PostgreSQL][postgres], [SQLite][sqlite], and more. NoSQL databases are a somewhat heterogeneous mix of databases. Some NoSQL databases converse entirely in [JSON (JavaScript Object Notation)][json]], and store _documents_ instead of tables (e.g., [MongoDB][mongo], [CouchDB][couch]), while others are simple _key-value_ stores, that use keys (e.g. `myblobofdata`) to point to a blob of data (e.g., [Redis][redis], [LevelDB][leveldb]). We covered database interfaces from R in [a post last May](https://ropensci.org/blog/2015/05/20/database-interfaces/).

A database holds data in a structured way. This structure can vary quite a bit (see above), but the salient point is that any database you have locally can also go online with a few bells and whistles to allow a user to interact with the database.

## <a href="#online" name="online">#</a> The power of online data

For some science fields, perhaps all their raw data to answer questions comes from the internet, whereas it used to come from books pre 21st century. For them, this is a non-issue. However, there are many fields - ecology, geology, archeology, etc. - that collect a lot of new data literally in the field, i.e., outdoors. This is essential work. Much of this new data collected is in the form of physical things stored in lab notebooks, spreadhseets, museums, and libraries. Metadata and data describing this material is moving online at a fast pace.

Many science questions or at least parts of questions can likely utilize data online, whether it be metadata or full text of published articles, climate data, biological specimen data, or molecular sequence data. Some fields, like macroecology, almost entirely use data collected by other people to ask questions at large spatial scales like _How general is the relationship between area sampled and biological diversity (i.e., number of species)?_. This question benefits from a lot of data.

As there are some scientists that may use data only from the internet for their work, some scientists may still benefit from _some_ online data use. One example is taxonomic data for biologists. Whether a biologist is studying a single species or a community of 1000 species, they often do not know the current state of the higher taxonomy, that is, the groupings above the species level that organize biological diversity. There are a large number of online databases for taxonomic information, many of which are avaiallable programatically through the [taxize R package][taxize]. Even if a biologist collects all her data in the field or in the lab, she still is likely to use the internet at some point in her research process to get information on her study species. For example, she may wonder _What is known about what my study species eats?_ - this may lead to a Google search, then an online database of information on the species. As the list of species for which one wants information grows longer, the repetitiveness of the task becomes more apparent, and the utility of a programatic approach becomes more appealing.

## <a href="#find" name="find">#</a> Finding data online

Where to find data online will be highly dependent on your field. Regardless of field, perhaps all na√Øve data searches begin by Googling keywords for the data of interest. This is a great way to start, but eventually you may need a more specific search tool. For example, if you are interested in scholarly texts, Google Scholar, Web of Science, and similar tools will allow better filtering to get the items desired.

This first step of web browsing is an important part of the data acquisition process. Although much scholarly content is behind paywalls, the metadata is almost always freely shared. This makes finding content online much easier. However, once found, content behind paywalls can lead to extreme frustration.

Special note should be taken of search engines that allow programmatic access to their data. Google, for example, has a great web interface for searching for pretty much anything. Although they do provide programmatic access to some of their data, they don't for the scholarly search engine ([https://scholar.google.com/][gschol]). On the other hand, GBIF is a clearinghouse for biodiversity records from all over the globe - like Google, they have a great web interface, but unlike Google, they allow programmatic access to data. It is this ability to have a nice web interface combined with programmatic access that makes a search engine so effective.

Below is a non-exhaustive list of open search engines that provide programmatic access.

<table>
<thead>
<tr>
<th style="text-align:left;"> Search engine   </th>
<th style="text-align:left;"> Category  </th>
<th style="text-align:left;"> Link     </th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;"> <em>Global Biodiversity Information Facility</em> </td>
<td style="text-align:left;"> Data </td>
<td style="text-align:left;"> <a href="http://www.gbif.org/">http://www.gbif.org/</a> </td>
</tr>
<tr>
<td style="text-align:left;"> <em>Crossref</em> </td>
<td style="text-align:left;"> Text </td>
<td style="text-align:left;"> <a href="http://crossref.org">http://crossref.org</a>       </td>
</tr>
<tr>
<td style="text-align:left;"> <em>NCBI</em> </td>
<td style="text-align:left;"> Data/Text </td>
<td style="text-align:left;"> <a href="http://ncbi.org">http://ncbi.org</a>       </td>
</tr>
<tr>
<td style="text-align:left;"> <em>Data.gov</em> </td>
<td style="text-align:left;"> Data </td>
<td style="text-align:left;"> <a href="http://data.gov">http://data.gov</a>       </td>
</tr>
</tbody>
</table>

## <a href="#move" name="move">#</a> When to move from the browser to R

Let's consider two scenarios.

In the first scenario, our scientist Jane wants to ask if there are more occurrences of a single bird species in North America or South America. She needs to get species occurrence data for one species, and for two continents. She hears about [GBIF][gbif] through a friend, and decides to check it out. Upon landing on the GBIF website, she figures out what she wants after navigating through a few pages, enters a search term for her species limiting to occurrences in North America, and downloads the data as a `.csv` file. She repeats the process for South America.

In the second scenario, Jill wants to ask if abundance distributions by latitude differ among species. Jill is interested in a larger sample size than Jane, about 500 species. She heard about GBIF through Jane, and decided to visit the site to get a sense for what data were available. She decided that data were appropriate, and started to download data for each species. After about 6 species, Jill had a nagging feeling that there must be a better way -- why repeat what you can automate. With a progamming language, Jill can repeat the download task for all 500 species with the same set of code, and pass in her species list to the code that gets the data. In fact, the code is pretty simple:

```{r}
library('rgbif')
splist <- c('Aphelandra acanthus','Gongora dressleri',
'Phleum alpinum','Indigofera floribunda','Himantoglossum calcaratum',
'Hypochaeris variegata','Calyptranthes rostellata','Lupinus altimontanus',
'Solanum candidum','Begonia hispidissima')
keys <- name_backbone(splist)
occ_search(keys)
```

```r
## Occ. found [2482598 (255927), 2492010 (1436233), 2498387 (416768)]
## Occ. returned [2482598 (5), 2492010 (5), 2498387 (5)]
## No. unique hierarchies [2482598 (1), 2492010 (1), 2498387 (1)]
## No. media records [2482598 (5), 2492010 (5), 2498387 (5)]
## Args [taxonKey=2482598,2492010,2498387, limit=5, fields=minimal]
## First 10 rows of data from 2482598
##
##                  name       key decimalLatitude decimalLongitude
## 1 Cyanocitta stelleri 891781350           37.74           -122.5
## 2 Cyanocitta stelleri 891051562           38.94           -120.0
## 3 Cyanocitta stelleri 891042142           37.24           -122.0
## 4 Cyanocitta stelleri 891047537           37.87           -122.2
## 5 Cyanocitta stelleri 891051134           35.19           -111.6
```

In the first scenario, Jane doesn't gain much by getting her data programmatically in R. She can quickly do it in the web browser. Especially if she is not very comfortable with programming, the web browser workflow works perfect. In the second scenario, Jill realized how long it would take in the browser, and even if she had some code to learn, getting the data programatically was worth it.

This is not to say once you move away from the browser to the command line, you should stay on the command line. The most effective data discovery is a combination of web browsing for initial discovery, and programmatic access for the repetitive tasks.

## <a href="#diss" name="diss">#</a> Disappearing data

Despite the benefits of online data, there are potential costs. There are a variety of things to consider with online data: whether the data exists any longer, the speed at which the server/database online provide the data, and your internet connection speed. Any of these things can influence the speed at which you get data back to your machine.

### <a href="#linkrot" name="linkrot">#</a> Link rot

Link rot should be taken into consideration in science. Many have thought hard about this problem, and have introduced the idea of the [persistent identifier][persistid]. This can be any alphanumeric string, but often takes the form of a [URI (Universal Resource Identifier)][uri]. One type of persistent identifier is the [DOI (Digital Object identifier)][doi], which is probably the most widely known type of persistent identifier. To quickly get a sense for the power of a persistent identifier, go to [http://doi.org/10.1371/journal.pone.0086169](http://doi.org/10.1371/journal.pone.0086169). Although the publisher of this paper (Public Library of Science) can change their URL for the article, the DOI for the article (`10.1371/journal.pone.0086169`) should always be resolvable to the current location for the article on the web.

Mostly, individual scientists don't deal with providing and worrying about identifiers and link rot. However, more and more you can share your various scientific outputs, and in locations that will provide a persistent identifier.

### <a href="#servers" name="servers">#</a> Servers

One thing out of scientists control is what goes on the server side. Think of your data acquisition process like ordering a coffee: the barista is the server, and you are the client. There are certain things out of your control that the barista does that affect the coffee you get. Likewise, computer servers are doing data manipulation/processing/caching/etc. that can change the data you get back; and sometimes they are completely down/unavailable. You will likely interact with a server via a client for a particularly programming language. Ideally, the maintainer of that client will keep the client up to date with changes on the server.

## <a href="#change" name="change">#</a> Database content and interfaces can change

Although interacting with online databases can be a great advantage to your science, there are a few gotchas. These fall in to two categories: 1) interface changes; and 2) content changes.

### <a href="#def" name="def">#</a> Interface

Every online database needs to provide some way for the user to inspect the database. Let's call this the "database interface". The database interface, like any other piece of software, can change through time as the software developer's decide on a better interface. The interface is not only what you see in a browser but how you programatically talk to the database. If you are accessing an online database with a script written in R or Python, and the programmatic way of interfacing to the database changes, your workflow can break. This is a good thing to be aware of when you access online databases - changes are beyond your control. If you are using an online database that many others use, it is likely that someone has done the work of creating a set of functions, or a library, that you can use to access the database.

### <a href="#def" name="def">#</a> Content

Most online databases do not stagnate, but grow as more data is added to them. Just like Wikipedia grows though time as new pages are added and new editors edit a page, online databases incorporate new data as they become available. Addtional data is additional knowledge. However, as content changes, even a single additional row in the database, your analysis can be altered. That is, if your code grabs data from an online database on May 5, 2012, then you query the database again one month later on June 5, 2012, you may get a different response from the database. This is of course not surprising.

However, this is a concern for reproducbility. Ther are a number of options when it comes to scientific reproducibility in the context of changing online database content:

* **Cache your data** Ideally this is the raw data that the online database provides, which your scripts then process to pass down through your workflow. However, this could be somewhat processed data to make it more consumable by others. Ideally you would provide this data along with your published paper, or as a stand alone dataset.
* **Data provider stores a cache of your data request** Databases can be versioned so that each change is stored, sort of like you can look back through versions of each file on Dropbox. This is the same with any database online. However, in practice online database rarely allow you to cache your data request, and if they do it's for maybe 24 hours or a few weeks. For science, ideally we would want indefinite caching. However, this means infrastructure - which is isn't hard to make, but hard to sustain.

## <a href="#sustain" name="sustain">#</a> Database sustainability

Serving up a database on the internet today is trivial in terms of costs and ease of setup (see next section). Often the limiting factor is cost, as hosting does add up over time, and as increased server sizes are needed. There are various solutions to the funding problem. First, if you are at a university they may give you free server space for the database. Second, small grants of a few thousand dollars could probably pay for a few years of hosting costs. Third, integrating your data into a larger data provider is a good route, and your data may be more likely to be seen. For example, if you have species occurrence record data, instead of providing your data yourself, you can submit the data to the [Global Biodiversity Information Facility (GBIF)][gbif], a global warehouse for these kind of data.

## <a href="#apis" name="apis">#</a> Think APIs first

It is probably natural, given a set of data, to think first about the GUI, or the Graphical User Interface - how users will interact with the data visually. However, to have a wider impact, if we think in terms of a web of data sets that are programatically writable and readable, we can create a much better data ecosystem. In other words, a great way to limit the use of your data is to completely ignore programmatic access, and only provide browser based methods of interacting with the data. Unfortunately, far too many scientific and government databases completely ignore programmatic access.

## <a href="#diy" name="diy">#</a> Roll your own database

If you already have a database you don't have to do a lot more work to make it available online programmatically. You only need to do three additional things: i) make your database available online, ii) provide a programmatic way to access the data, and optionally iii) make a web frontend for people to interact with your database. There are multitude of frameworks out there to easily hook up a server to your database (e.g., [Heroku][heroku], [AWS EC2][ec2], [Rackspace][rackspace], etc.). Many framework make it easy to make an API for programmatic access, including [Sinatra][sinatra], [Rails][rails], [Tastypie][pie], etc. There are many frameworks that are easy to use to slap on a web frontend.

If you have some data, but you don't already have them in a database, you may want to start by learning databases with [SQLite][sqlite], which is a super light weight SQL database. When it comes time to have your database serve data to the public on the web, you likely will want to change to something like [PostgreSQL][postgres].

If the above is too advanced, just getting data out on the web in any fashion is a great start.

[gbif]: http://gbif.org
[uri]: https://en.wikipedia.org/wiki/Uniform_Resource_Identifier
[doi]: https://en.wikipedia.org/wiki/Digital_object_identifier
[taxize]: https://github.com/ropensci/taxize
[gbif]: http://www.gbif.org/
[sqlite]: https://www.sqlite.org/
[postgres]: http://www.postgresql.org/
[mysql]: https://www.mysql.com/
[persistid]: https://en.wikipedia.org/wiki/Persistent_identifier
[json]: https://en.wikipedia.org/wiki/JSON
[mongo]: https://www.mongodb.org/
[couch]: http://couchdb.apache.org/
[redis]: http://redis.io/
[leveldb]: http://leveldb.org/
[gschol]: https://scholar.google.com/
[heroku]: https://heroku.com/
[ec2]: https://aws.amazon.com/ec2/
[rackspace]: https://www.rackspace.com/
[sinatra]: http://www.sinatrarb.com/
[Rails]: http://rubyonrails.org/
[pie]: http://tastypieapi.org/
