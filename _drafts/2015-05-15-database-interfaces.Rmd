---
name: database-interfaces
layout: post
title: Database interfaces
date: 2015-05-15
authors:
  - name: Scott Chamberlain
tags:
- R
- database
- key-value
- sql
- nosql
---

```{r echo=FALSE}
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  warning = FALSE,
  message = FALSE
)
```

There are many different databases. The most familiar are row-column SQL databases like MySQL, SQLite, or PostgreSQL. Another type of database is the key-value store, which as a concept is very simple: you save a value specified by a key, and you can retrieve a value by its key. One more type is the document database, which instead of storing rows and columns, stores blobs of text or even binary files. The key-value and document types fall under the NoSQL umbrella. As there are mature R clients for many SQL databases, and [dplyr](https://github.com/hadley/dplyr) is a great generic interface to SQL backends  (see [`dplyr` vignettes](http://cran.rstudio.com/web/packages/dplyr/) for an intro), we won't delve into SQL clients here.

What is the difference among SQL, NoSQL, key-value, etc.? A diagram may be helpful:

![diagram](databases_diagram/databases_diagram.jpg)

NoSQL is often interpreted as _Not only SQL_ - meaning a database that is called a NoSQL database may contain some notion of row-column storage, but other details diverge from traditional SQL databases. See [Wikipedia](http://en.wikipedia.org/wiki/NoSQL) for more information.

Current rOpenSci database tools:

* [elastic](https://github.com/ropensci/elastic) (document database) (on CRAN)
* [sofa](https://github.com/ropensci/sofa) (document database)
* [solr](https://github.com/ropensci/solr) (document database) (on CRAN)
* [etseed](https://github.com/ropensci/etseed) (key-value store)
* [rrlite](https://github.com/ropensci/rrlite) (key-value store)
* [rerddap](https://github.com/ropensci/rerddap) (SQL database as a service, open source) (on CRAN)
* [ckanr](https://github.com/ropensci/ckanr) (SQL database as a service, open source)
* [nodbi](https://github.com/ropensci/nodbi) (DBI, but for NoSQL DB's)

If you aren't already using databases, why care about databases? We'll answer this through a number of use cases:

* Use case 1: Let's say you are producing a lot of data in your lab - millions of rows of data. Storing all this data in `.xls` or `.csv` files would definitely get cumbersome. If the data is traditional row-column spreadsheet data, a SQL database is perfect, perhaps PostgreSQL. Putting your data in a database allows the data to scale up easily while maintaining speedy data access, many tables can be linked together if needed, and more. Of course if you're data will always fit in memory of the machine you're working on, a database may be too much complexity. 
* Use case 2: You already have data in a database, whether SQL or NoSQL. Of course the it makes sense to interface with the database from R instead of e.g., exporting files from the database, then into R. 
* Use case 3: A data provider gives dumps of data that you need for your research/work problem. You download the data and it's hundreds of `.csv` files. It sure would be nice to be able to efficiently search this data. Simple searches like _return all records with variable X < 10_ are ideal for a SQL database. If instead the files are blobs of XML, JSON, or something else non-tabular, a document database is ideal. * * Use case 4: You need to perform more complicated searches than SQL can support. Some NoSQL databases have very powerful search engines, e.g., Elasticsearch.
* Use case 5: Sometimes you just need to cache stuff. Caching is a good use case for key-value stores. Let's say you are requesting data from a database online, and you want to make a derivative data thing from the original data, but you don't want to lose the original data. Simply storing the original data on disk in files is easy and does the job. Sometimes though, you may need something more structured. Redis and etcd are two key-value stores we make clients for and can make caching easy. Another use for caching is to avoid repeating time-consuming queries or queries that may cost money.  

## Get devtools

We'll need `devtools` to install some of these packages, as not all are on CRAN. If you are on Windows, see [these notes](https://github.com/hadley/devtools#updating-to-the-latest-version-of-devtools).

```{r eval=FALSE}
install.packages("devtools")
```

## elastic

[elastic](https://github.com/ropensci/elastic) - Interact with Elasticsearch.

```{r eval=FALSE}
install.packages("elastic")
```

```{r}
library("elastic")
```

`elastic` is a powerful document database with a built in query engine. It speaks JSON by default, has a nice HTTP API, which we use to communicate with `elastic` from R. What's great about `elastic` over e.g., `Solr` is that you don't have to worry about specifying a schema for your data. You can simply put data in, and then query on that data. Of course you can [specify configuration settings](http://www.elastic.co/guide/en/elasticsearch/reference/current/setup-configuration.html).

### Example

In a quick example, here's going from a data.frame in R, putting data into `elastic`, then querying on that data.

```{r}
library("ggplot2")
invisible(connect())
res <- docs_bulk(diamonds, "diam")
```

About 54K records in Elasticsearch for the dataset.

```{r}
count("diam")
```

We don't have time to go through hardly any of the diverse and powerful Elasticsearch query interface, so as an example, let's plot the price of diamonds in $300 buckets using the [Elasticsearch aggregations search API](http://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations.html)

```{r tidy=FALSE}
aggs <- '{
    "aggs": {
        "pricebuckets" : {
           "histogram" : {
               "field" : "price",
               "interval" : 300
           }
        }
    }
}'
res <- Search("diam", body = aggs, size = 0)
df <- do.call("rbind.data.frame", res$aggregations$pricebuckets$buckets)
ggplot(df, aes(key, doc_count)) + 
  geom_bar(stat = "identity") + 
  theme_grey(base_size = 20) + 
  labs(x = "Price", y = "Count")
```

We have a package in developmented called [elasticdsl](https://github.com/ropensci/elasticdsl) that follows the lead of the Python client [elasticsearch-dsl-py](https://github.com/elastic/elasticsearch-dsl-py) to allow native R based ways to specify queries. The package focuses on querying for data, whereas other operations will remain in the lower level `elastic` client.

## sofa

[sofa](https://github.com/ropensci/sofa) - Interact with CouchDB.

```{r eval=FALSE}
devtools::install_github("ropensci/sofa")
```

```{r}
library("sofa")
```

### Example

Connect to your running CouchDB instance:

```{r}
?cushion
```

Create a database

```{r eval=FALSE}
db_create(dbname = 'sofadb')
```

Create a document in that database

```{r eval=FALSE}
doc_create('{"name":"sofa","beer":"IPA"}', dbname = "sofadb", docid = "a_beer")
```

Get the document

```{r eval=FALSE}
doc_get(dbname = "sofadb", docid = "a_beer")
```

There's a similar interface to inserting data within R directly into CouchDB, just as with Elasticsearch above. There's lots more to do in `sofa`, including adding ability to do map-reduce. 

## solr

[solr](https://github.com/ropensci/solr) - A client for interacting with Solr.

`solr` focuses on reading data from Solr engines. We are working on adding functionality for working with more Solr features, including writing documents. Adding support for writing to `solr` is a bit trickier than reading data, since writing data requires specifying a schema. 

```{r eval=FALSE}
install.packages("solr")
```

```{r}
library("solr")
```

### Example

A quick example using a remote Solr server the Public Library of Science search engine. 

```{r}
solr_search(q = '*:*', fl = c('id', 'journal', 'publication_date'), base = 'http://api.plos.org/search', verbose = FALSE)
```

`solr` is quite useful in R since it is a common search engine that is often exposed as is, so that you can pop this `solr` R client into your script or package and (hopefully) not have to worry about how to query the Solr service.

## etseed

[etseed](https://github.com/ropensci/etseed) is an R client for the [etcd](https://github.com/coreos/etcd) key-value store, developed by the folks at [coreos](https://coreos.com/), written in [Go](https://golang.org/).

This package is still in early days, and isn't exactly the fastest option in the bunch here - bought changes in `etcd` should help. 

```{r eval=FALSE}
devtools::install_github("ropensci/etseed")
```

```{r}
library("etseed")
```

> A note before we go through an example. etcd has a particular way of specifying keys, in that you have to prefix a key by a forward slash, like `/foobar` instead of `foobar`. 

### Example

Save a value to a key

```{r}
create(key = "/mykey", value = "this is awesome")
```

Fetch the value given a key

```{r}
key(key = "/mykey")
```

### rrlite

[rrlite](https://github.com/ropensci/rrlite) - An R client for the Redis C library [rlite](https://github.com/seppo0010/rlite)

```{r eval=FALSE}
devtools::install_github("ropensci/rrlite")
```

```{r}
library("rrlite")
```

No need to start up a server as with some of the other clients since rlite is a serverless engine. 

### Example

Here, we initialize, then put 20 values into rlite, assigned to the key `foo`, then retrieve the values by the same key.

```{r}
r <- RedisAPI::rdb(rrlite::hirlite)
r$set("foo", runif(20))
r$get("foo")
```

This is a good candidate for using within other R packages for more sophisticated caching than simply writing to disk, and is especially easy since you users aren't required to spin up a server as with normal Redis, or other DB's like CouchDB, MongoDB, etc. 

## rerddap

[rerddap](https://github.com/ropensci/rerddap) - A general purpose R client for any ERDDAP server.

ERDDAP servers 

```{r eval=FALSE}
install.packages("rerddap")
```

```{r}
library("rerddap")
```

ERDDAP is a server built on top of [OPenDAP](http://www.opendap.org/). NOAA serve many differen datasets through ERDDAP servers. Through ERDDAP, you can get gridded data (see `griddap()`), which lets you query from gridded datasets, or table data (see `tabledap()`) which lets you query from tabular datasets. ERDDAP is open source, so anyone can use it to serve data. 

`rerddap` by default grabs NetCDF files, a binary compressed file type that should be faster to download, and take up less disk space, than other formats (e.g., `csv`). However, this means that you need a client library for NetCDF files - but not to worry, we use `ncdf` by default (for which there are CRAN binaries for all platforms), but you can choose to use `ncdf4` (binaries only for some platforms).

### Example

In this example, we search for gridded datasets

```{r}
ed_search(query = 'size', which = "grid")
```

Get more information on a single dataset of interest

```{r}
info('noaa_esrl_027d_0fb5_5d38')
```

Then fetch the dataset

```{r}
griddap('noaa_esrl_027d_0fb5_5d38',
        time = c('2012-07-01', '2012-09-01'),
        latitude = c(21, 19),
        longitude = c(-80, -76)
)
```

There are many different ERDDAP servers, see the function `servers()` for help.

More information on ERDDAP: []()

## ckanr

[ckanr](https://github.com/ropensci/ckanr) - A general purpose R client for any CKAN server.

[CKAN](http://ckan.org/) is similar to ERDDAP in being an open source system to store and provide data via web services (and web interface, but we don't need that here). CKAN bills themself as a _open-source data portal platform_.

```{r eval=FALSE}
devtools::install_github("ropensci/ckanr")
```

```{r}
library("ckanr")
```

### Example

> Examples use the CKAN server at http://data.techno-science.ca

Show changes in a CKAN server

```{r}
changes(limit = 10, as = "table")[, 1:2]
```

Search for data packages

```{r}
package_search(q = '*:*', rows = 2, as = "table")$results[, 1:7]
```

More information on CKAN: [http://docs.ckan.org/en/latest/contents.html](http://docs.ckan.org/en/latest/contents.html)

## nodbi

[nodbi](https://github.com/ropensci/nodbi) - Like the DBI package, but for document and key-value databases.

`nodbi` has five backends at the moment:

* Redis
* etcd
* MongoDB
* CouchDB
* Elasticsearch

`nodbi` is still in early development, but that means it's a good time to give your input. What are use cases you can think of for this package?  What database do you think should be added as a backend?

```{r eval=FALSE}
devtools::install_github("ropensci/nodbi")
```

```{r}
library("nodbi")
```

### Example

We'll use MongoDB to store some data, then pull it back out. First, start up your mongo server, then intialize the connection

```
mongod
```

```{r}
(src <- src_mongo())
```

Insert data

```{r}
library("ggplot2")
diamonds$cut <- as.character(diamonds$cut)
diamonds$color <- as.character(diamonds$color)
diamonds$clarity <- as.character(diamonds$clarity)
docdb_create(src, key = "diam", value = diamonds)
```

Pull data back out

```{r}
res <- docdb_get(src, "diam")
head(res)
```

Data is identical:

```{r}
identical(diamonds, res)
```

## Further reading

* [DataCarpentry SQL tutorial][dcsql]

[dcsql]: https://github.com/datacarpentry/R-ecology/blob/gh-pages/06-r-and-sql.Rmd
