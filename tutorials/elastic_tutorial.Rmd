---
title: elastic tutorial
layout: tutorial
packge_version: 0.6.0
---

```{r, eval=TRUE, echo=FALSE}
knitr::opts_chunk$set(
	fig.path = "../assets/tutorial-images/elastic/",
	warning = FALSE,
	message = FALSE,
	comment = "#>"
)
```

`elastic` is an R client for [Elasticsearch](https://www.elastic.co/products/elasticsearch). This tutorial is an introduction to the package.

<section id="installation">

## Installation

You can install from CRAN

```{r eval=FALSE}
install.packages("elastic")
```

Or the development version from GitHub

```{r eval=FALSE}
install.packages("devtools")
devtools::install_github("ropensci/elastic")
```

Then load the package

```{r}
library("elastic")
```

<section id="usage">

## Elasticsearch info

+ [Elasticsearch home page](https://www.elastic.co/products/elasticsearch)
+ [API docs](http://www.elastic.co/guide/en/elasticsearch/reference/current/index.html)

## Install Elasticsearch

* [Elasticsearch installation help](http://www.elastic.co/guide/en/elasticsearch/reference/current/_installation.html)

__Unix (linux/osx)__

Replace `2.3.2` with the version you are working with.

+ Download zip or tar file from Elasticsearch [see here for download](https://www.elastic.co/downloads/elasticsearch), e.g., `curl -L -O https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-2.3.2.tar.gz`
+ Uncompress it: `tar -xvf elasticsearch-2.3.2.tar.gz`
+ Move it: `sudo mv elasticsearch-2.3.2 /usr/local`
+ Navigate to /usr/local: `cd /usr/local`
+ Add shortcut: `sudo ln -s elasticsearch-2.3.2 elasticsearch`

On OSX, you can install via Homebrew: `brew install elasticsearch`

__Windows__

Windows users can follow the above, but unzip the zip file instead of uncompressing the tar file.

## Start Elasticsearch

* Navigate to elasticsearch: `cd /usr/local/elasticsearch`
* Start elasticsearch: `bin/elasticsearch`

I create a little bash shortcut called `es` that does both of the above commands in one step (`cd /usr/local/elasticsearch && bin/elasticsearch`).

__Note:__ Windows users should run the `elasticsearch.bat` file

## Initialize connection

The function `connect()` is used before doing anything else to set the connection details to your remote or local elasticsearch store. The details created by `connect()` are written to your options for the current session, and are used by `elastic` functions.

```{r}
connect()
```

On package load, your base url and port are set to `http://127.0.0.1` and `9200`, respectively. You can of course override these settings per session or for all sessions.

## Get some data

Elasticsearch has a bulk load API to load data in fast. The format is pretty weird though. It's sort of JSON, but would pass no JSON linter. I include a few data sets in `elastic` so it's easy to get up and running, and so when you run examples in this package they'll actually run the same way (hopefully).

I have prepared a non-exported function useful for preparing the weird format that Elasticsearch wants for bulk data loads (see below). See `elastic:::make_bulk_plos` and `elastic:::make_bulk_gbif`.

### Shakespeare data

Elasticsearch provides some data on Shakespeare plays. I've provided a subset of this data in this package. Get the path for the file specific to your machine:

```{r}
shakespeare <- system.file("examples", "shakespeare_data.json", package = "elastic")
```

Then load the data into Elasticsearch:

```{r eval=FALSE}
docs_bulk(shakespeare)
```

If you need some big data to play with, the shakespeare dataset is a good one to start with. You can get the whole thing and pop it into Elasticsearch (beware, may take up to 10 minutes or so.):

```sh
curl -XGET http://www.elasticsearch.org/guide/en/kibana/current/snippets/shakespeare.json > shakespeare.json
curl -XPUT localhost:9200/_bulk --data-binary @shakespeare.json
```

### Public Library of Science (PLOS) data

A dataset inluded in the `elastic` package is metadata for PLOS scholarly articles. Get the file path, then load:

```{r eval=FALSE}
plosdat <- system.file("examples", "plos_data.json", package = "elastic")
docs_bulk(plosdat)
```

### Global Biodiversity Information Facility (GBIF) data

A dataset inluded in the `elastic` package is data for GBIF species occurrence records. Get the file path, then load:

```{r eval=FALSE}
gbifdat <- system.file("examples", "gbif_data.json", package = "elastic")
docs_bulk(gbifdat)
```

GBIF geo data with a coordinates element to allow `geo_shape` queries

```{r eval=FALSE}
gbifgeo <- system.file("examples", "gbif_geo.json", package = "elastic")
docs_bulk(gbifgeo)
```

### More data sets

There are more datasets formatted for bulk loading in the `ropensci/elastic_data` GitHub repository. Find it at [https://github.com/ropensci/elastic_data](https://github.com/ropensci/elastic_data)

## Search

Search the `plos` index and only return 1 result

```{r}
Search(index="plos", size=1)$hits$hits
```

Search the `plos` index, and the `article` document type, sort by title, and query for _antibody_, limit to 1 result

```{r}
Search(index="plos", type="article", sort="title", q="antibody", size=1)$hits$hits
```

## URL based search

A new function in `v0.4` is `Search_uri()`, where the search is defined entirely in the URL itself.
This is especially useful for cases in which `POST` requests are forbidden, e.g, on a server that
prevents `POST` requests for security reasons (which the function `Search()` uses)

Basic search

```{r}
Search_uri(index = "plos", size = 1)$hits$hits
```

Sorting

```{r}
res <- Search_uri(index = "shakespeare", type = "act", sort = "speaker:desc", fields = 'speaker')
sapply(res$hits$hits, "[[", c("fields", "speaker"))
```

### A bool query

```{r}
mmatch <- '{
 "query": {
   "bool" : {
     "must_not" : {
       "range" : {
         "speech_number" : {
           "from" : 1, "to": 5
}}}}}}'
sapply(Search(index="shakespeare", body=mmatch)$hits$hits, function(x) x$`_source`$speech_number)
```

### Fuzzy query

Fuzzy query on numerics

```{r}
fuzzy <- list(query = list(fuzzy = list(speech_number = list(value = 7, fuzziness = 4))))
Search(index="shakespeare", body=fuzzy)$hits$total
```

### Range query

With numeric

```{r}
body <- list(query=list(range=list(decimalLongitude=list(gte=1, lte=3))))
Search('gbif', body=body)$hits$total
```

With dates

```{r}
body <- list(query=list(range=list(eventDate=list(gte="2012-01-01", lte="now"))))
Search('gbif', body=body)$hits$total
```

### More-like-this query (more_like_this can be shortened to mlt)

```{r}
body <- '{
 "query": {
   "more_like_this": {
     "fields": ["abstract","title"],
     "like_text": "and then",
     "min_term_freq": 1,
     "max_query_terms": 12
   }
 }
}'
Search('plos', body=body)$hits$total
```

### Highlighting

```{r}
body <- '{
 "query": {
   "query_string": {
     "query" : "cell"
   }
 },
 "highlight": {
   "fields": {
     "title": {"number_of_fragments": 2}
   }
 }
}'
out <- Search('plos', 'article', body=body)
out$hits$total
```

```{r}
sapply(out$hits$hits, function(x) x$highlight$title[[1]])[8:10]
```

### Scrolling search - instead of paging

```{r}
Search('shakespeare', q="a*")$hits$total
res <- Search(index = 'shakespeare', q="a*", scroll="1m")
res <- Search(index = 'shakespeare', q="a*", scroll="1m", search_type = "scan")
length(scroll(scroll_id = res$`_scroll_id`)$hits$hits)
```

## Bulk load from R objects

A new feature in `v0.4` is loading data into Elasticsearch via the bulk API (faster than via the
normal route) from R objects (data.frame, or list). E.g.:

Using a pretty large data.frame, at 53K rows, load `ggplot2` package first

```{r}
library("ggplot2")
res <- invisible(docs_bulk(diamonds, "diam"))
Search(index = "diam")$hits$total
```

## Get documents

Get document with `id=1`

```{r}
docs_get(index='plos', type='article', id=1)
```

Get certain fields

```{r}
docs_get(index='plos', type='article', id=1, fields='id')
```

## Get multiple documents at once

Same index and type, different document ids

```{r}
docs_mget(index="plos", type="article", id=3:4)
```

Different indeces, types, and ids

```{r}
docs_mget(index_type_id=list(c("plos","article",1), c("gbif","record",1)))$docs[[1]]
```

## Raw JSON data

You can optionally get back raw `json` from `Search()`, `docs_get()`, and `docs_mget()` setting parameter `raw=TRUE`.

For example:

```{r}
(out <- docs_mget(index="plos", type="article", id=5:6, raw=TRUE))
```

Then parse

```{r}
jsonlite::fromJSON(out)
```



<section id="citing">

## Citing

> Scott Chamberlain (2016). elastic: General Purpose Interface to Elasticsearch. R package version 0.6.0.
  http://cran.rstudio.com/package=elastic



<section id="license_bugs">

## License and bugs

* License: [MIT](http://opensource.org/licenses/MIT)
* Report bugs at [our GitHub repo for elastic](https://github.com/ropensci/elastic/issues?state=open)


[Back to top](#top)
